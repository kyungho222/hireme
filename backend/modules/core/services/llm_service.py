import os
from datetime import datetime
from typing import Any, Dict, List

import httpx
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

try:
    import openai
except ImportError:
    openai = None


class LLMService:
    def __init__(self):
        # OpenAI ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = "gpt-4o-mini"

        # Ollama ì„¤ì • (ë°±ì—…)
        self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.ollama_model = "gpt-oss:20b"

        # Gemini ì„¤ì • (ë¹„í™œì„±í™”)
        self.gemini_api_key = None
        self.gemini_model = "gemini-1.5-pro"
        self.gemini_base_url = "https://generativelanguage.googleapis.com/v1beta/models"

        # ì‚¬ìš©í•  LLM ê²°ì • - OpenAI ìš°ì„ , ì—†ìœ¼ë©´ Ollama
        if self.openai_api_key:
            self.primary_llm = "openai"
            print(f"[LLMService] === LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘ ===")
            print(f"[LLMService] OpenAI ëª¨ë“œë¡œ ì‹¤í–‰")
            print(f"[LLMService] OpenAI ëª¨ë¸: {self.openai_model}")
            print(f"[LLMService] === LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ===")
        else:
            self.primary_llm = "ollama"
            print(f"[LLMService] === LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘ ===")
            print(f"[LLMService] Ollama ëª¨ë“œë¡œ ì‹¤í–‰ (OpenAI API í‚¤ ì—†ìŒ)")
            print(f"[LLMService] Ollama URL: {self.ollama_base_url}")
            print(f"[LLMService] Ollama ëª¨ë¸: {self.ollama_model}")
            print(f"[LLMService] === LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ ===")

    async def chat_completion(self, messages: List[Dict[str, str]], max_tokens: int = 300, temperature: float = 1.0) -> str:
        """LLM APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if self.primary_llm == "openai":
                return await self._openai_chat_completion(messages, max_tokens, temperature)
            else:
                return await self._ollama_chat_completion(messages, max_tokens, temperature)
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _openai_chat_completion(self, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> str:
        """OpenAI APIë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ì™„ì„±"""
        try:
            if not openai:
                return "OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            client = openai.OpenAI(api_key=self.openai_api_key)

            # ì†ë„ ìµœì í™”: ì¬ì‹œë„ ë¡œì§ ì œê±°, íƒ€ì„ì•„ì›ƒ ì„¤ì •
            try:
                # max_tokens ìµœì†Œê°’ ë³´ì¥ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì‘ë‹µì´ ì˜ë¦¼)
                safe_max_tokens = max(max_tokens, 500)  # ìµœì†Œê°’ì„ 500ìœ¼ë¡œ ì¦ê°€

                # OpenAI API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ 15ì´ˆë¡œ ì¦ê°€)
                response = client.chat.completions.create(
                    model=self.openai_model,
                    messages=messages,
                    max_completion_tokens=safe_max_tokens,
                    temperature=temperature,
                    timeout=15.0  # 15ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¦ê°€
                )

            except Exception as e:
                print(f"[LLMService] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                raise e

            # choices ìœ íš¨ì„± ê²€ì‚¬
            if not response or not hasattr(response, 'choices') or len(response.choices) == 0:
                print(f"[LLMService] OpenAI ë¹ˆ choices ë°°ì—´ ê°ì§€, ê¸°ë³¸ JSON ì‘ë‹µ ë°˜í™˜")
                print(f"[LLMService] ì‘ë‹µ êµ¬ì¡°: {response}")
                return '{"intent": "chat", "response": "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggested_tool": null, "suggested_action": null, "params": {}, "confidence": 0.5}'

            # finish_reason í™•ì¸
            choice = response.choices[0]
            if choice.finish_reason == 'length':
                print(f"[LLMService] OpenAI ì‘ë‹µì´ í† í° ê¸¸ì´ ì œí•œìœ¼ë¡œ ì˜ë¦¼ (finish_reason: length)")
                print(f"[LLMService] í˜„ì¬ max_tokens: {safe_max_tokens}, ë” í° ê°’ìœ¼ë¡œ ì¬ì‹œë„ í•„ìš”")
                # í† í° ê¸¸ì´ ì œí•œìœ¼ë¡œ ì˜ë¦° ê²½ìš° ë” í° í† í°ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    retry_response = client.chat.completions.create(
                        model=self.openai_model,
                        messages=messages,
                        max_completion_tokens=safe_max_tokens * 2,  # 2ë°°ë¡œ ì¦ê°€
                        temperature=temperature,
                        timeout=15.0
                    )
                    if retry_response.choices and retry_response.choices[0].message.content:
                        content = retry_response.choices[0].message.content
                        print(f"[LLMService] ì¬ì‹œë„ ì„±ê³µ: ì‘ë‹µ ê¸¸ì´ {len(content)}")
                        return content
                except Exception as retry_e:
                    print(f"[LLMService] ì¬ì‹œë„ ì‹¤íŒ¨: {retry_e}")

            # content ìœ íš¨ì„± ê²€ì‚¬
            if not choice.message or not choice.message.content:
                print(f"[LLMService] OpenAI ë¹ˆ content ê°ì§€, ê¸°ë³¸ JSON ì‘ë‹µ ë°˜í™˜")
                print(f"[LLMService] choice êµ¬ì¡°: {choice}")
                print(f"[LLMService] finish_reason: {choice.finish_reason}")
                return '{"intent": "chat", "response": "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggested_tool": null, "suggested_action": null, "params": {}, "confidence": 0.5}'

            content = choice.message.content

            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            if not content or content.strip() == "":
                print(f"[LLMService] OpenAI ë¹ˆ ì‘ë‹µ ê°ì§€, ê¸°ë³¸ JSON ì‘ë‹µ ë°˜í™˜")
                return '{"intent": "chat", "response": "ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "suggested_tool": null, "suggested_action": null, "params": {}, "confidence": 0.5}'

            print(f"[LLMService] OpenAI ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(content)})")
            return content

        except Exception as e:
            print(f"[LLMService] OpenAI API ì˜¤ë¥˜: {str(e)}")
            return f"OpenAI API ì˜¤ë¥˜: {str(e)}"

    async def _gemini_chat_completion(self, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> str:
        """Gemini APIë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ì™„ì„±"""
        try:
            # Gemini API ìš”ì²­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            contents = []

            for message in messages:
                contents.append({
                    "parts": [{"text": message["content"]}]
                })

            payload = {
                "contents": contents,
                "generationConfig": {
                    "temperature": temperature,
                    "topK": 40,
                    "topP": 0.95,
                    "maxOutputTokens": max_tokens,
                }
            }

            url = f"{self.gemini_base_url}/{self.gemini_model}:generateContent?key={self.gemini_api_key}"

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()

                result = response.json()
                if "candidates" in result and len(result["candidates"]) > 0:
                    content = result["candidates"][0]["content"]["parts"][0]["text"]
                    print(f"[LLMService] Gemini ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(content) if content else 0})")
                    return content
                else:
                    return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"[LLMService] Gemini API ì˜¤ë¥˜: {str(e)}")
            return "Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _ollama_chat_completion(self, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> str:
        """Ollama APIë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ì™„ì„±"""
        try:
            # Ollama API í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ë³€í™˜
            ollama_messages = []
            for msg in messages:
                if msg["role"] == "user":
                    ollama_messages.append({"role": "user", "content": msg["content"]})
                elif msg["role"] == "assistant":
                    ollama_messages.append({"role": "assistant", "content": msg["content"]})
                elif msg["role"] == "system":
                    ollama_messages.append({"role": "system", "content": msg["content"]})

            # Ollama API í˜¸ì¶œ
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.ollama_base_url}/api/chat",
                    json={
                        "model": self.ollama_model,
                        "messages": ollama_messages,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "num_predict": max_tokens
                        }
                    },
                    timeout=120.0
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result.get("message", {}).get("content", "")
                    print(f"[LLMService] Ollama ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(content) if content else 0})")
                    return content
                else:
                    print(f"[LLMService] Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                    return f"Ollama API ì˜¤ë¥˜: {response.status_code}"

        except Exception as e:
            print(f"[LLMService] Ollama API ì˜¤ë¥˜: {str(e)}")
            return f"Ollama API ì˜¤ë¥˜: {str(e)}"

    async def analyze_plagiarism_suspicion(self,
                                    original_resume: Dict[str, Any],
                                    similar_resumes: List[Dict[str, Any]],
                                    document_type: str = "ìì†Œì„œ") -> Dict[str, Any]:
        """
        í‘œì ˆ ì˜ì‹¬ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            original_resume (Dict[str, Any]): ì›ë³¸ ë¬¸ì„œ
            similar_resumes (List[Dict[str, Any]]): ìœ ì‚¬í•œ ë¬¸ì„œë“¤
            document_type (str): ë¬¸ì„œ íƒ€ì… ("ì´ë ¥ì„œ" ë˜ëŠ” "ìì†Œì„œ")

        Returns:
            Dict[str, Any]: í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ê²°ê³¼
        """
        try:
            print(f"[LLMService] === í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ì‹œì‘ ===")

            # ìì†Œì„œì˜ ê²½ìš° applicant_idë¡œ ì§€ì›ì ì´ë¦„ì„ ì¡°íšŒí•´ì•¼ í•¨
            if document_type == "ìì†Œì„œ":
                original_name = original_resume.get('basic_info_names') or original_resume.get('name')
                if not original_name:
                    # applicant_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì§€ì›ìì˜ ì´ë¦„ì„ í‘œì‹œ
                    applicant_id = original_resume.get('applicant_id')
                    if applicant_id:
                        original_name = f"ì§€ì›ìID_{applicant_id}"
                    else:
                        original_name = "Unknown"
            else:
                original_name = original_resume.get('name', 'Unknown')
            print(f"[LLMService] ì›ë³¸ {document_type}: {original_name}")
            print(f"[LLMService] ìœ ì‚¬í•œ {document_type} ìˆ˜: {len(similar_resumes)}")

            if not similar_resumes:
                print(f"[LLMService] ìœ ì‚¬í•œ {document_type}ê°€ ì—†ìŒ - LOW ì˜ì‹¬ë„ ë°˜í™˜")
                return {
                    "success": True,
                    "suspicion_level": "LOW",
                    "suspicion_score": 0.0,
                    "suspicion_score_percent": 0,
                    "analysis": f"ìœ ì‚¬í•œ {document_type}ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í‘œì ˆ ì˜ì‹¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.",
                    "recommendations": []
                }

            # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸ (API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            similarities = []
            for resume in similar_resumes:
                if "similarity_score" in resume:
                    similarities.append(resume["similarity_score"])
                elif "overall_similarity" in resume:
                    similarities.append(resume["overall_similarity"])
                else:
                    print(f"[LLMService] ê²½ê³ : ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - {resume.keys()}")
                    similarities.append(0.0)

            max_similarity = max(similarities) if similarities else 0.0
            print(f"[LLMService] ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜: {max_similarity:.3f}")

            # ì˜ì‹¬ë„ ë ˆë²¨ ê²°ì •
            if max_similarity >= 0.8:
                suspicion_level = "HIGH"
            elif max_similarity >= 0.6:
                suspicion_level = "MEDIUM"
            else:
                suspicion_level = "LOW"

            suspicion_score = max_similarity

            # LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„¸í•œ ë¶„ì„ ìƒì„±
            analysis = await self._generate_plagiarism_analysis(
                max_similarity, suspicion_level, len(similar_resumes), document_type, similar_resumes
            )

            recommendations = []

            print(f"[LLMService] ì˜ì‹¬ë„ ê²°ì • ì™„ë£Œ: {suspicion_level} (ì ìˆ˜: {suspicion_score:.3f})")
            print(f"[LLMService] === í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ì™„ë£Œ ===")

            return {
                "success": True,
                "suspicion_level": suspicion_level,
                "suspicion_score": suspicion_score,
                "suspicion_score_percent": int(suspicion_score * 100),
                "analysis": analysis,
                "recommendations": recommendations,
                "similar_count": len(similar_resumes),
                "analyzed_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"[LLMService] === í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ===")
            print(f"[LLMService] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            print(f"[LLMService] ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            import traceback
            print(f"[LLMService] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "suspicion_level": "UNKNOWN",
                "suspicion_score": 0.0,
                "suspicion_score_percent": 0,
                "analysis": "í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "analyzed_at": datetime.now().isoformat()
            }

    async def analyze_ideal_candidate(self, applicant_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì§€ì›ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ìƒì ì¸ ì¸ì¬ìƒ LLM ë¶„ì„ ìˆ˜í–‰

        Args:
            applicant_info (Dict): ì§€ì›ì ì •ë³´

        Returns:
            Dict: LLM ë¶„ì„ ê²°ê³¼ (5ê°œì˜ ì´ìƒì ì¸ ì¸ì¬ìƒ)
        """
        try:
            print(f"[LLMService] === ì´ìƒì ì¸ ì¸ì¬ìƒ LLM ë¶„ì„ ì‹œì‘ ===")
            print(f"[LLMService] ì§€ì›ì: {applicant_info.get('name', 'N/A')}")

            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._create_ideal_candidate_analysis_prompt(applicant_info)

            # OpenAI API í˜¸ì¶œ
            client = openai.OpenAI(api_key=self.openai_api_key)
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¸ì¬ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì§ë¬´ì— ìµœì í™”ëœ ì´ìƒì ì¸ ì¸ì¬ìƒ 5ê°œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ìš”ì²­ëœ ì •í™•í•œ í˜•ì‹ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ë” ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´ ë‚®ì¶¤
                max_tokens=1500
            )

            analysis_text = response.choices[0].message.content.strip()

            print(f"[LLMService] LLM ë¶„ì„ ì™„ë£Œ")
            print(f"[LLMService] === LLM ë¶„ì„ ê²°ê³¼ ===")
            print(analysis_text[:500] + "..." if len(analysis_text) > 500 else analysis_text)
            print(f"[LLMService] === LLM ë¶„ì„ ê²°ê³¼ ë ===")

            return {
                "success": True,
                "analysis": analysis_text,
                "target_applicant": applicant_info.get('name', 'N/A'),
                "analysis_type": "ideal_candidate",
                "recommendation_count": 5,
                "analyzed_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"[LLMService] ì´ìƒì ì¸ ì¸ì¬ìƒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": "ì´ìƒì ì¸ ì¸ì¬ìƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "analyzed_at": datetime.now().isoformat()
            }

    async def analyze_similar_applicants(self, target_applicant: Dict[str, Any],
                                       similar_applicants: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ìœ ì‚¬ ì§€ì›ìë“¤ì— ëŒ€í•œ LLM ë¶„ì„ ìˆ˜í–‰

        Args:
            target_applicant (Dict): ê¸°ì¤€ ì§€ì›ì ì •ë³´
            similar_applicants (List[Dict]): ìœ ì‚¬í•œ ì§€ì›ìë“¤ ì •ë³´

        Returns:
            Dict: LLM ë¶„ì„ ê²°ê³¼
        """
        try:
            print(f"[LLMService] === ìœ ì‚¬ ì§€ì›ì LLM ë¶„ì„ ì‹œì‘ ===")
            print(f"[LLMService] ê¸°ì¤€ ì§€ì›ì: {target_applicant.get('name', 'N/A')}")
            print(f"[LLMService] ìœ ì‚¬ ì§€ì›ì ìˆ˜: {len(similar_applicants)}")

            if not similar_applicants:
                return {
                    "success": False,
                    "message": "ë¶„ì„í•  ìœ ì‚¬ ì§€ì›ìê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._create_similar_applicants_analysis_prompt(target_applicant, similar_applicants)

            # OpenAI API í˜¸ì¶œ
            client = openai.OpenAI(api_key=self.openai_api_key)
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¸ì¬ ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìš”ì²­ëœ ì •í™•í•œ í˜•ì‹ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”. íŠ¹íˆ '### 3. ê° ìœ ì‚¬ ì§€ì›ìë³„ ìƒì„¸ ë¶„ì„' ì„¹ì…˜ì—ì„œ ê° ì§€ì›ìë§ˆë‹¤ ğŸ” í•µì‹¬ ê³µí†µì , ğŸ’¡ ì£¼ìš” íŠ¹ì§•, â­ ì¶”ì²œ ì´ìœ , ğŸ¯ ìœ ì‚¬ì„± ìš”ì¸ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # ë” ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´ ë‚®ì¶¤
                max_tokens=1000
            )

            analysis_text = response.choices[0].message.content.strip()

            print(f"[LLMService] LLM ë¶„ì„ ì™„ë£Œ")
            print(f"[LLMService] === LLM ë¶„ì„ ê²°ê³¼ ===")
            print(analysis_text[:500] + "..." if len(analysis_text) > 500 else analysis_text)
            print(f"[LLMService] === LLM ë¶„ì„ ê²°ê³¼ ë ===")

            return {
                "success": True,
                "analysis": analysis_text,
                "target_applicant": target_applicant,
                "similar_count": len(similar_applicants),
                "analyzed_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"[LLMService] ìœ ì‚¬ ì§€ì›ì ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": "ìœ ì‚¬ ì§€ì›ì ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "analyzed_at": datetime.now().isoformat()
            }

    def _create_similar_applicants_analysis_prompt(self, target_applicant: Dict[str, Any],
                                                 similar_applicants: List[Dict[str, Any]]) -> str:
        """ìœ ì‚¬ ì§€ì›ì ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ê¸°ì¤€ ì§€ì›ì ì •ë³´
        prompt = f"""ë‹¤ìŒ ê¸°ì¤€ ì§€ì›ìì™€ ìœ ì‚¬í•œ ì§€ì›ìë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì™œ ìœ ì‚¬í•œì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ê¸°ì¤€ ì§€ì›ì:**
- ì´ë¦„: {target_applicant.get('name', 'N/A')}
- ì§€ì›ì§ë¬´: {target_applicant.get('position', 'N/A')}
- ê²½ë ¥: {target_applicant.get('experience', 'N/A')}
- ê¸°ìˆ ìŠ¤íƒ: {target_applicant.get('skills', 'N/A')}
- ë¶€ì„œ: {target_applicant.get('department', 'N/A')}

**ìœ ì‚¬í•œ ì§€ì›ìë“¤:**
"""

        # ìœ ì‚¬ ì§€ì›ìë“¤ ì •ë³´
        for applicant in similar_applicants:
            prompt += f"""
{applicant['rank']}ìˆœìœ„. {applicant.get('name', 'N/A')}
- ì§€ì›ì§ë¬´: {applicant.get('position', 'N/A')}
- ê²½ë ¥: {applicant.get('experience', 'N/A')}
- ê¸°ìˆ ìŠ¤íƒ: {applicant.get('skills', 'N/A')}
- ë¶€ì„œ: {applicant.get('department', 'N/A')}
- ìœ ì‚¬ë„ ì ìˆ˜: {applicant.get('final_score', 0):.3f} (ë²¡í„°: {applicant.get('vector_score', 0):.3f}, í‚¤ì›Œë“œ: {applicant.get('keyword_score', 0):.3f})
"""

        prompt += """

**ë¶„ì„ ìš”ì²­:** ê° ìœ ì‚¬ ì§€ì›ìë³„ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”:

### 1. ê¸°ì¤€ ì§€ì›ìì™€ ê° ìœ ì‚¬ ì§€ì›ì ê°„ì˜ ê³µí†µì 
### 2. ìœ ì‚¬ì„±ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ íŠ¹ì„± ë¶„ì„
### 3. ê° ìœ ì‚¬ ì§€ì›ìë³„ ìƒì„¸ ë¶„ì„

- **[ì§€ì›ìëª…]**
    - ğŸ” **í•µì‹¬ ê³µí†µì **: [ê¸°ì¤€ ì§€ì›ìì™€ì˜ ì£¼ìš” ê³µí†µì  1ì¤„]
    - ğŸ’¡ **ì£¼ìš” íŠ¹ì§•**: [í•µì‹¬ ì—­ëŸ‰ì´ë‚˜ ê²½ë ¥ ìš”ì•½ 1ì¤„]
    - â­ **ì¶”ì²œ ì´ìœ **: [ì•„ë˜ 5ê°€ì§€ ê¸°ì¤€ ì¤‘ ê°€ì¥ í•´ë‹¹í•˜ëŠ” ìš”ì†Œë¥¼ ì„ íƒí•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
            * ê¸°ìˆ ì  ìš°ìœ„ì„±: ê¸°ì¤€ ì§€ì›ìë³´ë‹¤ ë” ë°œì „ëœ ê¸°ìˆ ì´ë‚˜ ì¶”ê°€ ê¸°ìˆ  ë³´ìœ 
            * ê²½ë ¥ ê¹Šì´: ë” ë§ì€ ê²½í—˜ì´ë‚˜ ê³ ê¸‰ í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²½í—˜
            * ì „ë¬¸ì„± í™•ì¥: ê¸°ì¤€ ì§€ì›ìì™€ ìœ ì‚¬í•˜ë©´ì„œë„ ì¶”ê°€ì ì¸ ì „ë¬¸ ì˜ì—­ ë³´ìœ 
            * ì„±ì¥ ê¶¤ì : ë” ë¹ ë¥¸ ì„±ì¥ì´ë‚˜ ë„ì „ì ì¸ ê²½í—˜ ì´ë ¥
            * ë¶€ê°€ ê°€ì¹˜: ê¸°ì¤€ ì§€ì›ìì—ê²Œ ì—†ëŠ” ì¶”ê°€ì ì¸ ìŠ¤í‚¬ì´ë‚˜ ê²½í—˜

            ì˜ˆì‹œ: "Kubernetesì™€ Docker ë“± DevOps ê¸°ìˆ ì„ ì¶”ê°€ë¡œ ë³´ìœ í•˜ì—¬ ê¸°ìˆ ì  ìš°ìœ„ì„±ì´ ìˆìŒ"
            ì˜ˆì‹œ: "10ë…„ ê²½ë ¥ìœ¼ë¡œ ì—”í„°í”„ë¼ì´ì¦ˆ ê¸‰ ì‹œìŠ¤í…œ êµ¬ì¶• ê²½í—˜ì´ í’ë¶€í•˜ì—¬ ê²½ë ¥ ê¹Šì´ê°€ ë›°ì–´ë‚¨"
    - ğŸ¯ **ìœ ì‚¬ì„± ìš”ì¸**: [ìœ ì‚¬ì„±ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ íŠ¹ì„±]

**ì‘ì„± ê·œì¹™:**
- ì¶”ì²œ ì´ìœ ëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ í•¨ê»˜ ì‘ì„±í•  ê²ƒ (ë‹¨ìˆœíˆ "ê²½ë ¥ ê¹Šì´", "ê¸°ìˆ ì  ìš°ìœ„ì„±" ë“±ì˜ ë‹¨ì–´ë§Œ ì“°ì§€ ë§ ê²ƒ)
- ê° í•­ëª©ì€ ê°„ê²°í•˜ê²Œ 1-2ì¤„ ì´ë‚´ë¡œ ì‘ì„±
- ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ìœ„ì£¼ë¡œ ì‘ì„±

**ê¸ˆì§€ì‚¬í•­:**
- ì¶”ì²œ ì´ìœ ì— "ê²½ë ¥ ê¹Šì´", "ê¸°ìˆ ì  ìš°ìœ„ì„±" ë“±ì˜ ë‹¨ì–´ë§Œ ë‹¨ë…ìœ¼ë¡œ ì‚¬ìš© ê¸ˆì§€
- ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ê¸°ìˆ ëª…, ê²½ë ¥ì—°ìˆ˜, í”„ë¡œì íŠ¸ ê²½í—˜ ë“±ê³¼ í•¨ê»˜ ì„œìˆ í•  ê²ƒ
"""

        return prompt

    def _create_ideal_candidate_analysis_prompt(self, applicant_info: Dict[str, Any]) -> str:
        """ì´ìƒì ì¸ ì¸ì¬ìƒ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ì§€ì›ì ì •ë³´ ìˆ˜ì§‘
        name = applicant_info.get('name', 'N/A')
        position = applicant_info.get('position', 'N/A')
        experience = applicant_info.get('experience', 'N/A')
        skills = applicant_info.get('skills', 'N/A')
        department = applicant_info.get('department', 'N/A')
        education = applicant_info.get('education', 'N/A')
        growth_background = applicant_info.get('growthBackground', '')
        motivation = applicant_info.get('motivation', '')
        career_history = applicant_info.get('careerHistory', '')
        resume_text = applicant_info.get('resume_text', '')
        resume_summary = applicant_info.get('resume_summary', '')
        resume_keywords = applicant_info.get('resume_keywords', [])

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì§€ì›ìì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì§ë¬´ì— ìµœì í™”ëœ ì´ìƒì ì¸ ì¸ì¬ìƒ 5ê°œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

**ì§€ì›ì ì •ë³´:**
- ì´ë¦„: {name}
- ì§€ì›ì§ë¬´: {position}
- ê²½ë ¥: {experience}
- ê¸°ìˆ ìŠ¤íƒ: {skills}
- ë¶€ì„œ: {department}
- í•™ë ¥: {education}

**ì§€ì›ì ë°°ê²½ ì •ë³´:**
- ì„±ì¥ë°°ê²½: {growth_background if growth_background else 'ì •ë³´ ì—†ìŒ'}
- ì§€ì›ë™ê¸°: {motivation if motivation else 'ì •ë³´ ì—†ìŒ'}
- ê²½ë ¥ì‚¬í•­: {career_history if career_history else 'ì •ë³´ ì—†ìŒ'}

**ì´ë ¥ì„œ ë‚´ìš©:**
- ì´ë ¥ì„œ ìš”ì•½: {resume_summary if resume_summary else 'ì •ë³´ ì—†ìŒ'}
- ì´ë ¥ì„œ í‚¤ì›Œë“œ: {', '.join(resume_keywords) if resume_keywords else 'ì •ë³´ ì—†ìŒ'}
- ì´ë ¥ì„œ ìƒì„¸: {resume_text[:500] + '...' if len(resume_text) > 500 else resume_text if resume_text else 'ì •ë³´ ì—†ìŒ'}

**ë¶„ì„ ìš”ì²­:**
ìœ„ ì§€ì›ìì˜ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ì§ë¬´({position})ì— ê°€ì¥ ì í•©í•œ ì´ìƒì ì¸ ì¸ì¬ìƒ 5ê°œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ ì •í™•í•œ í˜•ì‹ì„ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”.**

### ì´ìƒì ì¸ ì¸ì¬ìƒ ë¶„ì„ ê²°ê³¼

#### 1. [ì¸ì¬ìƒ ì œëª© 1]
- **í•µì‹¬ ì—­ëŸ‰**: [í•´ë‹¹ ì¸ì¬ìƒì˜ í•µì‹¬ ì—­ëŸ‰ ì„¤ëª…]
- **ì í•©ì„± ê·¼ê±°**: [ì§€ì›ì ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±]
- **ê¸°ëŒ€ íš¨ê³¼**: [í•´ë‹¹ ì¸ì¬ìƒì´ ì¡°ì§ì— ë¯¸ì¹  ê¸ì •ì  ì˜í–¥]

#### 2. [ì¸ì¬ìƒ ì œëª© 2]
- **í•µì‹¬ ì—­ëŸ‰**: [í•´ë‹¹ ì¸ì¬ìƒì˜ í•µì‹¬ ì—­ëŸ‰ ì„¤ëª…]
- **ì í•©ì„± ê·¼ê±°**: [ì§€ì›ì ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±]
- **ê¸°ëŒ€ íš¨ê³¼**: [í•´ë‹¹ ì¸ì¬ìƒì´ ì¡°ì§ì— ë¯¸ì¹  ê¸ì •ì  ì˜í–¥]

#### 3. [ì¸ì¬ìƒ ì œëª© 3]
- **í•µì‹¬ ì—­ëŸ‰**: [í•´ë‹¹ ì¸ì¬ìƒì˜ í•µì‹¬ ì—­ëŸ‰ ì„¤ëª…]
- **ì í•©ì„± ê·¼ê±°**: [ì§€ì›ì ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±]
- **ê¸°ëŒ€ íš¨ê³¼**: [í•´ë‹¹ ì¸ì¬ìƒì´ ì¡°ì§ì— ë¯¸ì¹  ê¸ì •ì  ì˜í–¥]

#### 4. [ì¸ì¬ìƒ ì œëª© 4]
- **í•µì‹¬ ì—­ëŸ‰**: [í•´ë‹¹ ì¸ì¬ìƒì˜ í•µì‹¬ ì—­ëŸ‰ ì„¤ëª…]
- **ì í•©ì„± ê·¼ê±°**: [ì§€ì›ì ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±]
- **ê¸°ëŒ€ íš¨ê³¼**: [í•´ë‹¹ ì¸ì¬ìƒì´ ì¡°ì§ì— ë¯¸ì¹  ê¸ì •ì  ì˜í–¥]

#### 5. [ì¸ì¬ìƒ ì œëª© 5]
- **í•µì‹¬ ì—­ëŸ‰**: [í•´ë‹¹ ì¸ì¬ìƒì˜ í•µì‹¬ ì—­ëŸ‰ ì„¤ëª…]
- **ì í•©ì„± ê·¼ê±°**: [ì§€ì›ì ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì í•©ì„±]
- **ê¸°ëŒ€ íš¨ê³¼**: [í•´ë‹¹ ì¸ì¬ìƒì´ ì¡°ì§ì— ë¯¸ì¹  ê¸ì •ì  ì˜í–¥]

### ì¢…í•© í‰ê°€
- **ì „ì²´ ì í•©ë„**: [ì§€ì›ìì˜ ì „ì²´ì ì¸ ì í•©ë„ í‰ê°€]
- **ì£¼ìš” ê°•ì **: [ì§€ì›ìì˜ ì£¼ìš” ê°•ì  3ê°€ì§€]
- **ê°œì„  ì œì•ˆ**: [ì§€ì›ìì˜ ê°œì„  ê°€ëŠ¥í•œ ì˜ì—­ 2-3ê°€ì§€]

**í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­:**
1. ë°˜ë“œì‹œ 5ê°œì˜ ì¸ì¬ìƒì„ ì œì‹œí•˜ì„¸ìš”
2. ê° ì¸ì¬ìƒì€ ì§€ì›ìì˜ ì‹¤ì œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
3. ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ì§€ ì•Šìœ¼ë©´ ì‹œìŠ¤í…œì—ì„œ íŒŒì‹±ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤
4. ì¸ì¬ìƒ ì œëª©ì€ ì§ë¬´ì™€ ê´€ë ¨ëœ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”
"""

        return prompt

    async def _generate_plagiarism_analysis(self,
                                          similarity_score: float,
                                          suspicion_level: str,
                                          similar_count: int,
                                          document_type: str,
                                          similar_documents: List[Dict[str, Any]]) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            similarity_score (float): ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
            suspicion_level (str): ìœ„í—˜ë„ ë ˆë²¨ (HIGH/MEDIUM/LOW)
            similar_count (int): ìœ ì‚¬í•œ ë¬¸ì„œ ê°œìˆ˜
            document_type (str): ë¬¸ì„œ íƒ€ì…
            similar_documents (List[Dict]): ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì˜ ìƒì„¸ ì •ë³´

        Returns:
            str: LLMì´ ìƒì„±í•œ ìƒì„¸ ë¶„ì„ í…ìŠ¤íŠ¸
        """
        try:
            print(f"[LLMService] LLM ê¸°ë°˜ í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ ì‹œì‘...")

            # ìœ ì‚¬ë„ ì ìˆ˜ë“¤ì„ ìˆ˜ì§‘
            similarity_details = []
            for doc in similar_documents[:3]:  # ìƒìœ„ 3ê°œë§Œ ë¶„ì„ì— í¬í•¨
                score = doc.get("similarity_score", doc.get("overall_similarity", 0.0))
                name = doc.get("basic_info_names", doc.get("name", "Unknown"))
                similarity_details.append(f"- {name}: {score:.1%} ìœ ì‚¬ë„")


            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {document_type} í‘œì ˆ ì˜ì‹¬ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

[ì—­í• ]
ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œì˜ ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ì„±ì„ í‰ê°€í•˜ëŠ” ê²€í†  ë³´ì¡°ìì…ë‹ˆë‹¤.
ê¸°ì¤€ ìì†Œì„œì˜ ì¼ë¶€ ë¬¸ì¥ì—ì„œ ì˜ë¯¸ ì¤‘ë³µì´ ê°ì§€ëœ ê²½ìš°, í‘œí˜„ êµ¬ì¡°ë‚˜ íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

[ì…ë ¥ ë°ì´í„°]
- ê¸°ì¤€ ìì†Œì„œ ë¬¸ì¥ ì¤‘ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ 1~2ê°œ
- ê° ë¬¸ì¥ì— ëŒ€í•œ ìœ ì‚¬ë„ ë ˆë²¨ (HIGH / MEDIUM / LOW)
- ê° ë¬¸ì¥ì— ëŒ€í•´ ìœ ì‚¬ íŒë‹¨ëœ ì´ìœ  (í‘œí˜„ êµ¬ì¡°, íë¦„, í‚¤ì›Œë“œ ë“±)

[ì‘ì„± ëª©í‘œ]
- ìœ ì‚¬ë„ ìˆ˜ì¹˜ë‚˜ ìœ ì‚¬ ìì†Œì„œ ê°œìˆ˜ëŠ” **ë§í•˜ì§€ ë§ˆì„¸ìš”**
- ê¸°ì¤€ ìì†Œì„œ ë‚´ **ìœ ì‚¬ ë¬¸ì¥**ê³¼ ê·¸ì— ëŒ€í•œ **ìœ ì‚¬ ì´ìœ **ë§Œ ê°„ê²°í•˜ê²Œ ì œì‹œ
- ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ì¤‘ë¦½ì  LLM í‰ê°€ ë¬¸ì¥ì„ ë„£ìœ¼ì„¸ìš” ("ê²€í†  ê¶Œì¥" ë“±)

[ì¶œë ¥ ì˜ˆì‹œ]

â€œâ€˜ê³ ê° ì¤‘ì‹¬ ì‚¬ê³ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.â€™ ë¬¸ì¥ì€ í‘œí˜„ êµ¬ì¡°ì™€ í•µì‹¬ ë‹¨ì–´ê°€ ë°˜ë³µë˜ì–´ HIGH ë“±ê¸‰ì˜ ìœ ì‚¬ì„±ì´ ê´€ì¸¡ë˜ì—ˆìŠµë‹ˆë‹¤.
ë˜í•œ â€˜í˜‘ì—…ì„ í†µí•´ ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ë©° ì„±ì¥í–ˆìŠµë‹ˆë‹¤.â€™ ë¬¸ì¥ë„ ìœ ì‚¬í•œ íë¦„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ MEDIUM ë“±ê¸‰ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ì¼ë¶€ ë¬¸ì¥ì—ì„œ ì˜ë¯¸ì  ì¤‘ë³µì´ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, í‘œì ˆ ì—¬ë¶€ì— ëŒ€í•œ ê²€í† ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.â€


"""

            # OpenAI API í˜¸ì¶œ
            client = openai.OpenAI(api_key=self.openai_api_key)
            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ í‘œì ˆ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì„ë² ë”© ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ í‘œì ˆ ì˜ì‹¬ë„ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
                max_tokens=200
            )

            analysis_text = response.choices[0].message.content.strip()

            # 3ì¤„ ì œí•œ ì²˜ë¦¬
            lines = analysis_text.split('\n')
            if len(lines) > 3:
                analysis_text = '\n'.join(lines[:3])
                print(f"[LLMService] LLM ì‘ë‹µì´ {len(lines)}ì¤„ì´ë¯€ë¡œ 3ì¤„ë¡œ ì œí•œë¨")

            print(f"[LLMService] LLM ê¸°ë°˜ í‘œì ˆ ë¶„ì„ ì™„ë£Œ (ê¸¸ì´: {len(analysis_text)})")

            return analysis_text

        except Exception as e:
            print(f"[LLMService] LLM ê¸°ë°˜ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
            if suspicion_level == "HIGH":
                return f"ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„({similarity_score:.1%})ì˜ {document_type}ê°€ {similar_count}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œì ˆ ì˜ì‹¬ë„ê°€ ë†’ì•„ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            elif suspicion_level == "MEDIUM":
                return f"ë†’ì€ ìœ ì‚¬ë„({similarity_score:.1%})ì˜ {document_type}ê°€ {similar_count}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í‘œì ˆ ì˜ì‹¬ë„ê°€ ë³´í†µ ìˆ˜ì¤€ì´ë¯€ë¡œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                return f"ì ì • ìˆ˜ì¤€ì˜ ìœ ì‚¬ë„({similarity_score:.1%})ì…ë‹ˆë‹¤. ìœ ì‚¬í•œ {document_type} {similar_count}ê°œê°€ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜ í‘œì ˆ ì˜ì‹¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤."
