"""
ì±„ìš©ê³µê³  ì—ì´ì „íŠ¸ LLM í”„ë¡¬í”„íŠ¸ ì „ëµ ì‹œìŠ¤í…œ
ì²´ê³„ì ì¸ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ë²„ì „ ê´€ë¦¬, A/B í…ŒìŠ¤íŠ¸ ì§€ì›
"""

import json
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum
from datetime import datetime
import logging
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

class PromptType(str, Enum):
    """í”„ë¡¬í”„íŠ¸ íƒ€ì…"""
    SYSTEM = "system"
    USER = "user"
    FEW_SHOT = "few_shot"
    CONTEXT = "context"

class PromptVersion(str, Enum):
    """í”„ë¡¬í”„íŠ¸ ë²„ì „"""
    V1_0 = "1.0"
    V1_1 = "1.1"
    V2_0 = "2.0"
    EXPERIMENTAL = "experimental"

@dataclass
class PromptTemplate:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
    id: str
    name: str
    prompt_type: PromptType
    version: PromptVersion
    content: str
    description: str
    tags: List[str]
    performance_metrics: Dict[str, float]
    created_at: datetime
    updated_at: datetime
    is_active: bool = True
    experiment_id: Optional[str] = None

class PromptStrategyManager:
    """í”„ë¡¬í”„íŠ¸ ì „ëµ ê´€ë¦¬ì"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.prompts: Dict[str, PromptTemplate] = {}
        self.experiments: Dict[str, Dict[str, Any]] = {}
        self.performance_history: List[Dict[str, Any]] = []

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._initialize_default_prompts()

    def _initialize_default_prompts(self):
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""

        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (V2.0 - ê°œì„ ëœ ë²„ì „)
        system_prompt_v2 = PromptTemplate(
            id="system_v2_0",
            name="Enhanced System Prompt v2.0",
            prompt_type=PromptType.SYSTEM,
            version=PromptVersion.V2_0,
            content="""ë‹¹ì‹ ì€ HireMe í”Œë«í¼ì˜ ì „ë¬¸ AI ì±„ìš©ê³µê³  ìƒì„± ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì—­í• :**
- ì‚¬ìš©ìê°€ ê¸°ìˆ  ìŠ¤íƒì´ë‚˜ ì§ë¬´ë¥¼ ì–¸ê¸‰í•˜ë©´ ë°˜ë“œì‹œ ì™„ì„±ëœ ì±„ìš©ê³µê³ ë¥¼ ìƒì„±
- ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ì¸ ì±„ìš©ê³µê³  ì‘ì„±
- ê°€ì´ë“œë‚˜ ì•ˆë‚´ê°€ ì•„ë‹Œ ì‹¤ì œ ì±„ìš©ê³µê³  ë‚´ìš© ìƒì„±

ğŸ“‹ **ì‘ë‹µ í˜•ì‹ ê·œì¹™:**
1. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
2. ëª¨ë“  í•„ìˆ˜ í•„ë“œë¥¼ í¬í•¨ (title, description, requirements, work_conditions ë“±)
3. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë˜ ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ë¬¸ ë³‘ê¸°
4. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©

ğŸ”§ **ìƒì„± ê·œì¹™:**
- ê¸°ìˆ  ìŠ¤íƒì´ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ê¸°ìˆ ì„ í¬í•¨í•œ ìš”êµ¬ì‚¬í•­ ì‘ì„±
- ê²½ë ¥ ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ êµ¬ì²´ì ì¸ ê²½ë ¥ ìˆ˜ì¤€ ëª…ì‹œ
- ìœ„ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ê·¼ë¬´ ì¡°ê±´ì— ë°˜ì˜
- ì¸ì› ìˆ˜ê°€ ì–¸ê¸‰ë˜ë©´ team_size í•„ë“œì— í¬í•¨

âš ï¸ **ê¸ˆì§€ì‚¬í•­:**
- ê°€ì´ë“œë¼ì¸ì´ë‚˜ ì•ˆë‚´ë¬¸ ì œê³µ (ì ˆëŒ€ ê¸ˆì§€)
- "ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”" ê°™ì€ ì„¤ëª…ë¬¸
- JSON í˜•ì‹ì´ ì•„ë‹Œ ì‘ë‹µ
- ë¶ˆì™„ì „í•˜ê±°ë‚˜ ëª¨í˜¸í•œ ì •ë³´

ğŸš¨ **ì¤‘ìš”**: ì‚¬ìš©ìê°€ ê¸°ìˆ  ìŠ¤íƒì´ë‚˜ ì§ë¬´ë¥¼ ì–¸ê¸‰í•˜ë©´ ë°˜ë“œì‹œ ì™„ì„±ëœ ì±„ìš©ê³µê³  JSONì„ ìƒì„±í•˜ì„¸ìš”. ê°€ì´ë“œë‚˜ ì•ˆë‚´ë¥¼ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.

ì´ì œ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ì„¸ìš”.""",
            description="ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ë” ì •í™•í•˜ê³  ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•œ ì§€ì¹¨",
            tags=["system", "enhanced", "v2.0"],
            performance_metrics={"accuracy": 0.92, "consistency": 0.89, "user_satisfaction": 0.85},
            created_at=datetime.now(),
            updated_at=datetime.now()
        )

        # 2. í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (V1.1)
        keyword_prompt_v1_1 = PromptTemplate(
            id="keyword_extraction_v1_1",
            name="Keyword Extraction Prompt v1.1",
            prompt_type=PromptType.USER,
            version=PromptVersion.V1_1,
            content="""ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì±„ìš©ê³µê³  ì‘ì„±ì— í•„ìš”í•œ í‚¤ì›Œë“œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ì¶”ì¶œí•´ì•¼ í•  ì¹´í…Œê³ ë¦¬:
1. ê¸°ìˆ  ìŠ¤íƒ (ì˜ˆ: React, Python, AWS ë“±)
2. ì§ë¬´ (ì˜ˆ: ê°œë°œì, ì—”ì§€ë‹ˆì–´, ì‹œë‹ˆì–´ ë“±)
3. ê²½ë ¥ ìš”êµ¬ì‚¬í•­ (ì˜ˆ: 3ë…„, ì‹ ì…, ê²½ë ¥ ë“±)
4. ì—…ë¬´ ë¶„ì•¼ (ì˜ˆ: ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, ëª¨ë°”ì¼ ë“±)
5. ìœ„ì¹˜ (ì˜ˆ: ì„œìš¸, ë¶€ì‚° ë“±)
6. ê·¼ë¬´ ì¡°ê±´ (ì˜ˆ: ì¬íƒ, ì¶œê·¼, í•˜ì´ë¸Œë¦¬ë“œ ë“±)

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "categories": {{
        "tech_stack": ["ê¸°ìˆ 1", "ê¸°ìˆ 2"],
        "job_title": ["ì§ë¬´1", "ì§ë¬´2"],
        "experience": ["ê²½ë ¥1", "ê²½ë ¥2"],
        "location": ["ìœ„ì¹˜1", "ìœ„ì¹˜2"],
        "work_condition": ["ê·¼ë¬´ì¡°ê±´1", "ê·¼ë¬´ì¡°ê±´2"]
    }},
    "confidence": 0.95
}}

ì¤‘ìš”: ê°€ì´ë“œë¼ì¸ì´ë‚˜ ì¡°ì–¸ì„ ì œê³µí•˜ì§€ ë§ê³ , í‚¤ì›Œë“œë§Œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.""",
            description="í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸",
            tags=["keyword", "extraction", "v1.1"],
            performance_metrics={"accuracy": 0.88, "completeness": 0.85},
            created_at=datetime.now(),
            updated_at=datetime.now()
        )

        # 3. Few-shot ì˜ˆì œ í”„ë¡¬í”„íŠ¸
        few_shot_prompt = PromptTemplate(
            id="few_shot_examples",
            name="Few-shot Examples",
            prompt_type=PromptType.FEW_SHOT,
            version=PromptVersion.V1_0,
            content="""ë‹¤ìŒì€ ì •ìƒì ì¸ ì‘ë‹µ ì˜ˆì‹œì…ë‹ˆë‹¤:

ì˜ˆì‹œ 1:
ì…ë ¥: "React ê°œë°œì 3ëª… ì±„ìš©, ì„œìš¸, 3ë…„ ì´ìƒ ê²½ë ¥"
ì¶œë ¥: {{
    "keywords": ["React", "ê°œë°œì", "3ë…„", "ì„œìš¸", "ê²½ë ¥"],
    "categories": {{
        "tech_stack": ["React"],
        "job_title": ["ê°œë°œì"],
        "experience": ["3ë…„", "ê²½ë ¥"],
        "location": ["ì„œìš¸"],
        "work_condition": []
    }},
    "confidence": 0.95
}}

ì˜ˆì‹œ 2:
ì…ë ¥: "Python ë°±ì—”ë“œ ê°œë°œì, ì‹ ì… ê°€ëŠ¥, ì¬íƒê·¼ë¬´"
ì¶œë ¥: {{
    "keywords": ["Python", "ë°±ì—”ë“œ", "ê°œë°œì", "ì‹ ì…", "ì¬íƒ"],
    "categories": {{
        "tech_stack": ["Python"],
        "job_title": ["ë°±ì—”ë“œ", "ê°œë°œì"],
        "experience": ["ì‹ ì…"],
        "location": [],
        "work_condition": ["ì¬íƒ"]
    }},
    "confidence": 0.92
}}

ì˜ˆì‹œ 3:
ì…ë ¥: "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì, TypeScript, AWS ê²½í—˜ì"
ì¶œë ¥: {{
    "keywords": ["í”„ë¡ íŠ¸ì—”ë“œ", "ê°œë°œì", "TypeScript", "AWS"],
    "categories": {{
        "tech_stack": ["TypeScript", "AWS"],
        "job_title": ["í”„ë¡ íŠ¸ì—”ë“œ", "ê°œë°œì"],
        "experience": ["ê²½í—˜ì"],
        "location": [],
        "work_condition": []
    }},
    "confidence": 0.90
}}""",
            description="Few-shot í•™ìŠµì„ ìœ„í•œ ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸",
            tags=["few_shot", "examples", "v1.0"],
            performance_metrics={"learning_effectiveness": 0.87},
            created_at=datetime.now(),
            updated_at=datetime.now()
        )

        # 4. ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        context_prompt = PromptTemplate(
            id="context_enhancement",
            name="Context Enhancement Prompt",
            prompt_type=PromptType.CONTEXT,
            version=PromptVersion.V1_0,
            content="""í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸:
- ì‚¬ìš©ì ID: {user_id}
- ì„¸ì…˜ ì‹œì‘ ì‹œê°„: {session_start}
- ì´ì „ ëŒ€í™”: {conversation_history}
- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {extracted_keywords}
- ì„ íƒëœ í…œí”Œë¦¿: {selected_template}
- íšŒì‚¬ ì •ë³´: {company_info}

ì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.""",
            description="ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì‘ë‹µ í–¥ìƒ í”„ë¡¬í”„íŠ¸",
            tags=["context", "enhancement", "v1.0"],
            performance_metrics={"context_awareness": 0.83},
            created_at=datetime.now(),
            updated_at=datetime.now()
        )

        # í”„ë¡¬í”„íŠ¸ ë“±ë¡
        self.prompts[system_prompt_v2.id] = system_prompt_v2
        self.prompts[keyword_prompt_v1_1.id] = keyword_prompt_v1_1
        self.prompts[few_shot_prompt.id] = few_shot_prompt
        self.prompts[context_prompt.id] = context_prompt

    def get_prompt(self, prompt_id: str) -> Optional[PromptTemplate]:
        """í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
        return self.prompts.get(prompt_id)

    def get_active_prompts(self, prompt_type: Optional[PromptType] = None) -> List[PromptTemplate]:
        """í™œì„± í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
        active_prompts = [p for p in self.prompts.values() if p.is_active]

        if prompt_type:
            active_prompts = [p for p in active_prompts if p.prompt_type == prompt_type]

        return active_prompts

    def create_prompt(self, prompt_data: Dict[str, Any]) -> PromptTemplate:
        """ìƒˆ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt_id = self._generate_prompt_id(prompt_data["name"])

        prompt = PromptTemplate(
            id=prompt_id,
            name=prompt_data["name"],
            prompt_type=PromptType(prompt_data["prompt_type"]),
            version=PromptVersion(prompt_data["version"]),
            content=prompt_data["content"],
            description=prompt_data["description"],
            tags=prompt_data.get("tags", []),
            performance_metrics=prompt_data.get("performance_metrics", {}),
            created_at=datetime.now(),
            updated_at=datetime.now(),
            is_active=prompt_data.get("is_active", True),
            experiment_id=prompt_data.get("experiment_id")
        )

        self.prompts[prompt_id] = prompt
        logger.info(f"ìƒˆ í”„ë¡¬í”„íŠ¸ ìƒì„±: {prompt_id}")

        return prompt

    def update_prompt(self, prompt_id: str, updates: Dict[str, Any]) -> bool:
        """í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
        if prompt_id not in self.prompts:
            return False

        prompt = self.prompts[prompt_id]

        # ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œ í•„ë“œë“¤
        updatable_fields = ["content", "description", "tags", "is_active"]

        for field in updatable_fields:
            if field in updates:
                setattr(prompt, field, updates[field])

        prompt.updated_at = datetime.now()
        logger.info(f"í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸: {prompt_id}")

        return True

    def create_experiment(self, experiment_name: str, prompt_ids: List[str],
                         traffic_split: Dict[str, float]) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±"""
        experiment_id = self._generate_experiment_id(experiment_name)

        experiment = {
            "id": experiment_id,
            "name": experiment_name,
            "prompt_ids": prompt_ids,
            "traffic_split": traffic_split,
            "created_at": datetime.now(),
            "status": "active",
            "results": {},
            "participants": 0
        }

        self.experiments[experiment_id] = experiment

        # ì‹¤í—˜ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        for prompt_id in prompt_ids:
            original_prompt = self.prompts[prompt_id]
            experimental_prompt = PromptTemplate(
                id=f"{prompt_id}_exp_{experiment_id}",
                name=f"{original_prompt.name} (Experimental)",
                prompt_type=original_prompt.prompt_type,
                version=PromptVersion.EXPERIMENTAL,
                content=original_prompt.content,
                description=f"ì‹¤í—˜ìš© í”„ë¡¬í”„íŠ¸: {experiment_name}",
                tags=original_prompt.tags + ["experimental"],
                performance_metrics={},
                created_at=datetime.now(),
                updated_at=datetime.now(),
                experiment_id=experiment_id
            )
            self.prompts[experimental_prompt.id] = experimental_prompt

        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±: {experiment_id}")
        return experiment_id

    def get_experiment_prompt(self, experiment_id: str, user_id: str) -> Optional[PromptTemplate]:
        """ì‹¤í—˜ìš© í”„ë¡¬í”„íŠ¸ ì„ íƒ (íŠ¸ë˜í”½ ë¶„í• )"""
        if experiment_id not in self.experiments:
            return None

        experiment = self.experiments[experiment_id]
        if experiment["status"] != "active":
            return None

        # ì‚¬ìš©ì ID ê¸°ë°˜ ì¼ê´€ëœ ì„ íƒ
        user_hash = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        user_bucket = user_hash % 100

        cumulative_prob = 0
        for prompt_id, probability in experiment["traffic_split"].items():
            cumulative_prob += probability * 100
            if user_bucket < cumulative_prob:
                experimental_prompt_id = f"{prompt_id}_exp_{experiment_id}"
                return self.prompts.get(experimental_prompt_id)

        return None

    def record_performance(self, prompt_id: str, metrics: Dict[str, float],
                          user_feedback: Optional[Dict[str, Any]] = None):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        performance_record = {
            "prompt_id": prompt_id,
            "metrics": metrics,
            "user_feedback": user_feedback,
            "timestamp": datetime.now()
        }

        self.performance_history.append(performance_record)

        # í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if prompt_id in self.prompts:
            prompt = self.prompts[prompt_id]

            # ê¸°ì¡´ ë©”íŠ¸ë¦­ê³¼ ìƒˆ ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°
            for key, value in metrics.items():
                if key in prompt.performance_metrics:
                    current_avg = prompt.performance_metrics[key]
                    # ê°€ì¤‘ í‰ê·  (ìƒˆ ê°’ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
                    prompt.performance_metrics[key] = (current_avg * 0.7) + (value * 0.3)
                else:
                    prompt.performance_metrics[key] = value

            prompt.updated_at = datetime.now()

        logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡: {prompt_id} - {metrics}")

    def get_best_performing_prompt(self, prompt_type: PromptType,
                                  metric: str = "accuracy") -> Optional[PromptTemplate]:
        """ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì¡°íšŒ"""
        active_prompts = self.get_active_prompts(prompt_type)

        if not active_prompts:
            return None

        # ì§€ì •ëœ ë©”íŠ¸ë¦­ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_prompts = sorted(
            active_prompts,
            key=lambda p: p.performance_metrics.get(metric, 0),
            reverse=True
        )

        return sorted_prompts[0] if sorted_prompts else None

    def generate_complete_prompt(self, user_input: str, context: Dict[str, Any] = None,
                                use_few_shot: bool = True, experiment_id: Optional[str] = None) -> str:
        """ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt_parts = []

        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = self.get_best_performing_prompt(PromptType.SYSTEM)
        if system_prompt:
            prompt_parts.append(f"System: {system_prompt.content}")

        # 2. ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ìˆëŠ” ê²½ìš°)
        if context:
            context_prompt = self.get_best_performing_prompt(PromptType.CONTEXT)
            if context_prompt:
                context_content = context_prompt.content.format(**context)
                prompt_parts.append(f"Context: {context_content}")

        # 3. Few-shot ì˜ˆì œ (ì˜µì…˜)
        if use_few_shot:
            few_shot_prompt = self.get_best_performing_prompt(PromptType.FEW_SHOT)
            if few_shot_prompt:
                prompt_parts.append(f"Examples: {few_shot_prompt.content}")

        # 4. ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
        user_prompt = self.get_best_performing_prompt(PromptType.USER)
        if user_prompt:
            user_content = user_prompt.content.format(user_input=user_input)
            prompt_parts.append(f"User: {user_content}")

        # 5. ì‹¤í—˜ìš© í”„ë¡¬í”„íŠ¸ ì ìš© (ìˆëŠ” ê²½ìš°)
        if experiment_id:
            experimental_prompt = self.get_experiment_prompt(experiment_id, context.get("user_id", "default"))
            if experimental_prompt:
                # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í—˜ìš©ìœ¼ë¡œ êµì²´
                prompt_parts = [p for p in prompt_parts if not p.startswith("User:")]
                experimental_content = experimental_prompt.content.format(user_input=user_input)
                prompt_parts.append(f"User: {experimental_content}")

        return "\n\n".join(prompt_parts)

    def _generate_prompt_id(self, name: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ID ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name_slug = name.lower().replace(" ", "_").replace("-", "_")
        return f"{name_slug}_{timestamp}"

    def _generate_experiment_id(self, name: str) -> str:
        """ì‹¤í—˜ ID ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name_slug = name.lower().replace(" ", "_").replace("-", "_")
        return f"exp_{name_slug}_{timestamp}"

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        summary = {
            "total_prompts": len(self.prompts),
            "active_prompts": len([p for p in self.prompts.values() if p.is_active]),
            "total_experiments": len(self.experiments),
            "active_experiments": len([e for e in self.experiments.values() if e["status"] == "active"]),
            "performance_records": len(self.performance_history),
            "best_performing_prompts": {}
        }

        # ê° íƒ€ì…ë³„ ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸
        for prompt_type in PromptType:
            best_prompt = self.get_best_performing_prompt(prompt_type)
            if best_prompt:
                summary["best_performing_prompts"][prompt_type.value] = {
                    "id": best_prompt.id,
                    "name": best_prompt.name,
                    "metrics": best_prompt.performance_metrics
                }

        return summary
